{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce9c4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id='gpt2' output=\"\\n\\nA: I'm good.\\n\\nQ: Did you get any good news or bad news?\\n\\nA: I didn't get any good news.\\n\\nQ: How did you get to the hospital?\\n\\nA:\"\n"
     ]
    }
   ],
   "source": [
    "from generator import generate\n",
    "from schemas import GenerateRequest, Model\n",
    "\n",
    "import httpx\n",
    "from httpx import AsyncClient\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer token-abc123\"\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    model_name=\"gpt2\",\n",
    "    args={\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 50,})\n",
    "\n",
    "prompt = \"hello, how are you?\"\n",
    "gen_prompt = GenerateRequest(prompt=prompt)\n",
    "\n",
    "async with AsyncClient() as client:\n",
    "    result = await generate(gen_prompt, model=model, client=client, headers=headers)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112de67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d77b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import HTTPException\n",
    "import yaml\n",
    "\n",
    "with open(\"../config.yaml\") as f:\n",
    "    model_config = yaml.safe_load(f)[\"models\"]\n",
    "\n",
    "def get_model_url(model_id: str) -> str:\n",
    "    if model_id not in model_config:\n",
    "        raise HTTPException(status_code=404, detail=\"Model not found\")\n",
    "    return model_config[model_id][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c509a99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': {'name': 'gpt2',\n",
       "  'url': 'http://localhost:8001',\n",
       "  'device': 'cuda',\n",
       "  'priority': 1},\n",
       " 'qwen': {'name': 'Qwen/Qwen2.5-0.5B-Instruct',\n",
       "  'url': 'http://localhost:8002',\n",
       "  'device': 'cpu',\n",
       "  'VLLM_CPU_OMP_THREADS_BIND': 1,\n",
       "  'VLLM_CPU_KVCACHE_SPACE': 1},\n",
       " 'llama': {'name': 'meta-llama/Llama-3.2-1B-Instruct',\n",
       "  'url': 'http://localhost:8003',\n",
       "  'device': 'cpu',\n",
       "  'VLLM_CPU_OMP_THREADS_BIND': 2,\n",
       "  'VLLM_CPU_KVCACHE_SPACE': 1}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7f6f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\n",
    "  \"models_health\": [\n",
    "    {\n",
    "      \"url\": \"http://localhost:8001\",\n",
    "      \"model_id\": \"gpt2\",\n",
    "      \"model_name\": \"gpt2\",\n",
    "      \"status\": \"healthy\"\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"http://localhost:8002\",\n",
    "      \"model_id\": \"qwen\",\n",
    "      \"model_name\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "      \"status\": \"healthy\"\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"http://localhost:8003\",\n",
    "      \"model_id\": \"llama\",\n",
    "      \"model_name\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "      \"status\": \"healthy\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca3b6fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'http://localhost:8001', 'model_id': 'gpt2', 'model_name': 'gpt2', 'status': 'healthy'}\n"
     ]
    }
   ],
   "source": [
    "for model in a[\"models_health\"]:\n",
    "    print(model)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3d9d42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models_outputs': [{'model_name': 'gpt2',\n",
       "   'prompt': 'How to grow a tree?',\n",
       "   'output': '\\n\\nMakes a nice, clean, and comfortable tree.\\n\\nCan grow in the summer months.\\n\\nCan be harvested and cut down for sale.\\n\\nCan be harvested and cut down for sale.\\n\\nCan be used as a fertilizer and can even be used as an energy source.\\n\\nCan be used as a fertilizer and can even be used as an energy source.\\n\\nCan be used in a high-intensity sport such as basketball.\\n\\nCan be used'},\n",
       "  {'model_name': 'Qwen/Qwen2.5-0.5B-Instruct',\n",
       "   'prompt': 'How to grow a tree?',\n",
       "   'output': ' Trees are plants, and like all plants, they require water, sunlight, nutrients, warmth, air, etc. All of this is what we put in the soil.\\nTo grow trees:\\n- Start with seedling or young saplings from your local nursery\\n- Water regularly (daily or twice daily)\\n- Feed with composted manure or composted horse manure\\n- Add fertilizer at flowering time using 20:1 liquid formula\\n- Make sure your tree has access to lots of'},\n",
       "  {'model_name': 'meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'prompt': 'How to grow a tree?',\n",
       "   'output': \" A step by step guide\\n\\nGrowing a tree can be a rewarding and rewarding experience. Here's a step-by-step guide to help you grow your very own tree.\\n\\n**Step 1: Choose the Right Tree**\\n\\n* **Select a tree species**: Look for trees that are suitable for your climate, soil type, and available space. Consider factors like growth rate, maintenance requirements, and disease resistance.\\n* **Consider the mature size**: Choose a tree that will grow to a size that fits your available\"}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"models_outputs\":[{\"model_name\":\"gpt2\",\"prompt\":\"How to grow a tree?\",\"output\":\"\\n\\nMakes a nice, clean, and comfortable tree.\\n\\nCan grow in the summer months.\\n\\nCan be harvested and cut down for sale.\\n\\nCan be harvested and cut down for sale.\\n\\nCan be used as a fertilizer and can even be used as an energy source.\\n\\nCan be used as a fertilizer and can even be used as an energy source.\\n\\nCan be used in a high-intensity sport such as basketball.\\n\\nCan be used\"},{\"model_name\":\"Qwen/Qwen2.5-0.5B-Instruct\",\"prompt\":\"How to grow a tree?\",\"output\":\" Trees are plants, and like all plants, they require water, sunlight, nutrients, warmth, air, etc. All of this is what we put in the soil.\\nTo grow trees:\\n- Start with seedling or young saplings from your local nursery\\n- Water regularly (daily or twice daily)\\n- Feed with composted manure or composted horse manure\\n- Add fertilizer at flowering time using 20:1 liquid formula\\n- Make sure your tree has access to lots of\"},{\"model_name\":\"meta-llama/Llama-3.2-1B-Instruct\",\"prompt\":\"How to grow a tree?\",\"output\":\" A step by step guide\\n\\nGrowing a tree can be a rewarding and rewarding experience. Here's a step-by-step guide to help you grow your very own tree.\\n\\n**Step 1: Choose the Right Tree**\\n\\n* **Select a tree species**: Look for trees that are suitable for your climate, soil type, and available space. Consider factors like growth rate, maintenance requirements, and disease resistance.\\n* **Consider the mature size**: Choose a tree that will grow to a size that fits your available\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c0b8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GGML_VERBOSE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f85eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU, compute capability 8.6, VMM: yes\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU) - 2688 MiB free\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /home/krasniuk-ai/model-rag-vote/Qwen_Qwen3-8B-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B\n",
      "llama_model_loader: - kv   3:                           general.basename str              = Qwen3\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\n",
      "llama_model_loader: - kv   7:                       qwen3.context_length u32              = 32768\n",
      "llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288\n",
      "llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  27:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  28:                      quantize.imatrix.file str              = /models_out/Qwen3-8B-GGUF/Qwen_Qwen3-...\n",
      "llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 252\n",
      "llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 137\n",
      "llama_model_loader: - type  f32:  145 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q6_K:   37 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.68 GiB (4.90 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 26\n",
      "load: token to piece cache size = 0.9311 MB\n",
      "print_info: arch             = qwen3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 36\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 12288\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.19 B\n",
      "print_info: general.name     = Qwen3 8B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  33 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  34 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  35 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  36 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 244 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
      "load_tensors: offloading 14 repeating layers to GPU\n",
      "load_tensors: offloaded 14/37 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  1556.70 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  3232.49 MiB\n",
      ".....................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 1024\n",
      "llama_context: n_ctx_per_seq = 1024\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.58 MiB\n",
      "create_memory: n_ctx = 1024 (padded)\n",
      "llama_kv_cache_unified: kv_size = 1024, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  32: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  33: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  34: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  35: dev = CUDA0\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =    56.00 MiB\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    88.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  144.00 MiB, K (f16):   72.00 MiB, V (f16):   72.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 2\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:      CUDA0 compute buffer size =   791.61 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    10.01 MiB\n",
      "llama_context: graph nodes  = 1374\n",
      "llama_context: graph splits = 290 (with bs=512), 47 (with bs=1)\n",
      "CUDA : ARCHS = 860 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '252', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.file_type': '15', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'qwen3.attention.value_length': '128', 'quantize.imatrix.chunks_count': '137', 'quantize.imatrix.file': '/models_out/Qwen3-8B-GGUF/Qwen_Qwen3-8B.imatrix', 'qwen3.attention.key_length': '128', 'general.architecture': 'qwen3', 'tokenizer.ggml.padding_token_id': '151643', 'general.basename': 'Qwen3', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- set tool_start = \"<tool_response>\" %}\\n    {%- set tool_start_length = tool_start|length %}\\n    {%- set start_of_message = message.content[:tool_start_length] %}\\n    {%- set tool_end = \"</tool_response>\" %}\\n    {%- set tool_end_length = tool_end|length %}\\n    {%- set start_pos = (message.content|length) - tool_end_length %}\\n    {%- if start_pos < 0 %}\\n        {%- set start_pos = 0 %}\\n    {%- endif %}\\n    {%- set end_of_message = message.content[start_pos:] %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {%- set content = message.content %}\\n        {%- set reasoning_content = \\'\\' %}\\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if \\'</think>\\' in message.content %}\\n                {%- set content = (message.content.split(\\'</think>\\')|last).lstrip(\\'\\\\n\\') %}\\n        {%- set reasoning_content = (message.content.split(\\'</think>\\')|first).rstrip(\\'\\\\n\\') %}\\n        {%- set reasoning_content = (reasoning_content.split(\\'<think>\\')|last).lstrip(\\'\\\\n\\') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n<think>\\\\n\\' + reasoning_content.strip(\\'\\\\n\\') + \\'\\\\n</think>\\\\n\\\\n\\' + content.lstrip(\\'\\\\n\\') }}\\n            {%- else %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- \\'<think>\\\\n\\\\n</think>\\\\n\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}', 'qwen3.context_length': '32768', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen3 8B', 'qwen3.attention.layer_norm_rms_epsilon': '0.000001', 'general.type': 'model', 'general.size_label': '8B', 'qwen3.block_count': '36', 'general.license': 'apache-2.0', 'qwen3.embedding_length': '4096', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen3.attention.head_count': '32', 'qwen3.feed_forward_length': '12288', 'qwen3.rope.freq_base': '1000000.000000', 'qwen3.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- messages[0].content + '\\n\\n' }}\n",
      "    {%- endif %}\n",
      "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
      "{%- for message in messages[::-1] %}\n",
      "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
      "    {%- set tool_start = \"<tool_response>\" %}\n",
      "    {%- set tool_start_length = tool_start|length %}\n",
      "    {%- set start_of_message = message.content[:tool_start_length] %}\n",
      "    {%- set tool_end = \"</tool_response>\" %}\n",
      "    {%- set tool_end_length = tool_end|length %}\n",
      "    {%- set start_pos = (message.content|length) - tool_end_length %}\n",
      "    {%- if start_pos < 0 %}\n",
      "        {%- set start_pos = 0 %}\n",
      "    {%- endif %}\n",
      "    {%- set end_of_message = message.content[start_pos:] %}\n",
      "    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\n",
      "        {%- set ns.multi_step_tool = false %}\n",
      "        {%- set ns.last_query_index = index %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set content = message.content %}\n",
      "        {%- set reasoning_content = '' %}\n",
      "        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n",
      "            {%- set reasoning_content = message.reasoning_content %}\n",
      "        {%- else %}\n",
      "            {%- if '</think>' in message.content %}\n",
      "                {%- set content = (message.content.split('</think>')|last).lstrip('\\n') %}\n",
      "        {%- set reasoning_content = (message.content.split('</think>')|first).rstrip('\\n') %}\n",
      "        {%- set reasoning_content = (reasoning_content.split('<think>')|last).lstrip('\\n') %}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "        {%- if loop.index0 > ns.last_query_index %}\n",
      "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
      "            {%- else %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "            {%- endif %}\n",
      "        {%- else %}\n",
      "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls %}\n",
      "            {%- for tool_call in message.tool_calls %}\n",
      "                {%- if (loop.first and content) or (not loop.first) %}\n",
      "                    {{- '\\n' }}\n",
      "                {%- endif %}\n",
      "                {%- if tool_call.function %}\n",
      "                    {%- set tool_call = tool_call.function %}\n",
      "                {%- endif %}\n",
      "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
      "                {{- tool_call.name }}\n",
      "                {{- '\", \"arguments\": ' }}\n",
      "                {%- if tool_call.arguments is string %}\n",
      "                    {{- tool_call.arguments }}\n",
      "                {%- else %}\n",
      "                    {{- tool_call.arguments | tojson }}\n",
      "                {%- endif %}\n",
      "                {{- '}\\n</tool_call>' }}\n",
      "            {%- endfor %}\n",
      "        {%- endif %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
      "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"/home/krasniuk-ai/model-rag-vote/Qwen_Qwen3-8B-Q4_K_M.gguf\",\n",
    "    n_ctx=1024,\n",
    "    # 2500MB, mb can do even more,\n",
    "    n_gpu_layers=14,\n",
    "    offload_kqv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61a75908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krasniuk-ai/model-rag-vote/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum ModelWrapper at line 757479 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load tokenizer from original Qwen\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen3-8B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model-rag-vote/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:814\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    810\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    812\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist or is not currently imported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    813\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[32m    817\u001b[39m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model-rag-vote/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2029\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2026\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2027\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2029\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2036\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2037\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2038\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2039\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2040\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model-rag-vote/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2261\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2259\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2261\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2262\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   2263\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m   2264\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load vocabulary from file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2265\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2266\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model-rag-vote/.venv/lib/python3.12/site-packages/transformers/models/qwen2/tokenization_qwen2_fast.py:129\u001b[39m, in \u001b[36mQwen2TokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, merges_file, tokenizer_file, unk_token, bos_token, eos_token, pad_token, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m unk_token = (\n\u001b[32m    119\u001b[39m     AddedToken(unk_token, lstrip=\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip=\u001b[38;5;28;01mFalse\u001b[39;00m, special=\u001b[38;5;28;01mTrue\u001b[39;00m, normalized=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(unk_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m unk_token\n\u001b[32m    122\u001b[39m )\n\u001b[32m    123\u001b[39m pad_token = (\n\u001b[32m    124\u001b[39m     AddedToken(pad_token, lstrip=\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip=\u001b[38;5;28;01mFalse\u001b[39;00m, special=\u001b[38;5;28;01mTrue\u001b[39;00m, normalized=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pad_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m pad_token\n\u001b[32m    127\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmerges_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model-rag-vote/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:111\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     fast_tokenizer = copy.deepcopy(tokenizer_object)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     fast_tokenizer = \u001b[43mTokenizerFast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[32m    114\u001b[39m     fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[31mException\u001b[39m: data did not match any variant of untagged enum ModelWrapper at line 757479 column 3"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer from original Qwen\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5bb9debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "from datetime import datetime\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Iterator, List, Optional, Union, Literal\n",
    "\n",
    "\n",
    "class Message(BaseModel):\n",
    "    name: str = \"message\"\n",
    "    role: Literal[\"user\", \"assistant\", \"system\"]\n",
    "    content: str = Field(default_factory=str)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"\\n{self.content}\\n\"\n",
    "    \n",
    "class LlmMessage(Message):\n",
    "    tool_calls: List[Dict] = Field(default_factory=list)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"\\n{self.content}\\n\"\n",
    "    \n",
    "    def model_dump(self, **kwargs):\n",
    "        base = super().model_dump(**kwargs)\n",
    "        base[\"formatted\"] = {\"role\": self.role, \"content\": self.content}\n",
    "        return base[\"formatted\"]\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.model_dump().items()\n",
    "\n",
    "class UserMessage(Message):\n",
    "    user_id: str = Field(default_factory=str)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"\\n{self.content}\\n\"\n",
    "    \n",
    "    def model_dump(self, **kwargs):\n",
    "        base = super().model_dump(**kwargs)\n",
    "        base[\"formatted\"] = {\"role\": self.role, \"content\": self.content}\n",
    "        return base[\"formatted\"]\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.model_dump().items()\n",
    "\n",
    "class SystemMessage(Message):\n",
    "    def __repr__(self):\n",
    "        return f\"\\n{self.content}\\n\"\n",
    "    \n",
    "    def model_dump(self, **kwargs):\n",
    "        base = super().model_dump(**kwargs)\n",
    "        base[\"formatted\"] = {\"role\": self.role, \"content\": self.content}\n",
    "        return base[\"formatted\"]\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.model_dump().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74bd8554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krasniuk-ai/model-rag-vote/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/krasniuk-ai/model-rag-vote/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Override attn_implementation='sdpa' to 'eager' as use_memory_efficient_attention=True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "path = 'Alibaba-NLP/gte-base-en-v1.5'\n",
    "device = torch.device('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    trust_remote_code=True,\n",
    "    unpad_inputs=True,\n",
    "    use_memory_efficient_attention=True,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9d5d33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "class BaseTool(ABC):\n",
    "    name: str = ''\n",
    "    description: str = ''\n",
    "    parameters: Union[List[dict], dict] = []\n",
    "\n",
    "    def __init__(self, cfg: Optional[dict] = None):\n",
    "        self.cfg = cfg or {}\n",
    "        if not self.name:\n",
    "            raise ValueError(\n",
    "                f'You must set {self.__class__.__name__}.name, either by @register_tool(name=...) or explicitly setting {self.__class__.__name__}.name'\n",
    "            )\n",
    "        if isinstance(self.parameters, dict):\n",
    "            if not is_tool_schema({'name': self.name, 'description': self.description, 'parameters': self.parameters}):\n",
    "                raise ValueError(\n",
    "                    'The parameters, when provided as a dict, must confirm to a valid openai-compatible JSON schema.')\n",
    "\n",
    "    @abstractmethod\n",
    "    def call(self, params: Union[str, dict], **kwargs) -> Union[str, list, dict]:\n",
    "        \"\"\"The interface for calling tools.\n",
    "\n",
    "        Each tool needs to implement this function, which is the workflow of the tool.\n",
    "\n",
    "        Args:\n",
    "            params: The parameters of func_call.\n",
    "            kwargs: Additional parameters for calling tools.\n",
    "\n",
    "        Returns:\n",
    "            The result returned by the tool, implemented in the subclass.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def function_info(self) -> dict:\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'description': self.description,\n",
    "            'parameters': self.parameters\n",
    "        }\n",
    "\n",
    "\n",
    "class NewsAPITool(BaseTool):\n",
    "    name: str = 'news_api_search'\n",
    "    description: str = 'Search for news using NewsAPI. Returns latest news articles with direct links.'\n",
    "\n",
    "    class Config:\n",
    "        api_key: str = \"9efd68b03f504c759af44000a347b287\"\n",
    "        max_retries: int = 2\n",
    "\n",
    "    parameters: dict = {\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'query': {\n",
    "                'type': 'string',\n",
    "                'description': 'Search query (e.g. \"Ukraine Russia ceasefire\")'\n",
    "            },\n",
    "            'language': {\n",
    "                'type': 'string',\n",
    "                'description': 'Language code (e.g. \"ru\", \"en\")',\n",
    "                'default': 'en'\n",
    "            },\n",
    "            'max_results': {\n",
    "                'type': 'integer',\n",
    "                'description': 'Number of results (1-30)',\n",
    "                'default': 3\n",
    "            },\n",
    "            'sort_by': {\n",
    "                'type': 'string',\n",
    "                'description': 'Sorting method: \"relevancy\", \"popularity\", or \"publishedAt\"',\n",
    "                'default': 'publishedAt'\n",
    "            }\n",
    "        },\n",
    "        'required': ['query'],\n",
    "    }\n",
    "\n",
    "    def call(self, params: dict) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Execute news search via NewsAPI\"\"\"\n",
    "        query = params['query']\n",
    "        language = params.get('language', 'en')\n",
    "        max_results = min(params.get('max_results', 3), 30)\n",
    "        sort_by = params.get('sort_by', 'publishedAt')\n",
    "\n",
    "        for attempt in range(self.Config.max_retries + 1):\n",
    "            try:\n",
    "                url = (\n",
    "                    f\"https://newsapi.org/v2/everything?\"\n",
    "                    f\"q={query}&\"\n",
    "                    f\"language={language}&\"\n",
    "                    f\"pageSize={max_results}&\"\n",
    "                    f\"sortBy={sort_by}&\"\n",
    "                    f\"apiKey={self.Config.api_key}\"\n",
    "                )\n",
    "\n",
    "                response = requests.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "\n",
    "                if data['status'] == 'ok':\n",
    "                    return self._format_results(data['articles'])\n",
    "\n",
    "                raise Exception(f\"API error: {data.get('message', 'Unknown error')}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt == self.Config.max_retries:\n",
    "                    return [{\n",
    "                        'error': f\"NewsAPI search failed after {self.Config.max_retries} attempts\",\n",
    "                        'details': str(e)\n",
    "                    }]\n",
    "                continue\n",
    "\n",
    "    def fetch_full_article_text(self, url: str) -> str:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10, headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "            })\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Try main content blocks\n",
    "            for tag in [\"article\", \"main\"]:\n",
    "                container = soup.find(tag)\n",
    "                if container:\n",
    "                    paragraphs = container.find_all(\"p\")\n",
    "                    if paragraphs:\n",
    "                        return \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "\n",
    "            # Fallback to all <p> tags\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            return \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"[Failed to fetch article text: {e}]\"\n",
    "\n",
    "    def _format_results(self, articles: List[Dict]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Standardize NewsAPI response and fetch full article text\"\"\"\n",
    "        formatted_json = []\n",
    "        formatted_text = []\n",
    "        for article in articles:\n",
    "            full_text = self.fetch_full_article_text(article[\"url\"])\n",
    "\n",
    "            formatted_json.append({\n",
    "                \"title\": article[\"title\"],\n",
    "                \"source\": article[\"source\"][\"name\"],\n",
    "                \"url\": article[\"url\"],\n",
    "                \"published_at\": article[\"publishedAt\"],\n",
    "                \"description\": article.get(\"description\"),\n",
    "                \"image_url\": article.get(\"urlToImage\")\n",
    "            })\n",
    "            formatted_text.append(full_text)\n",
    "\n",
    "        return formatted_json, formatted_text\n",
    "\n",
    "    def format_for_display(self, results: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"User-friendly results formatting\"\"\"\n",
    "        if results and 'error' in results[0]:\n",
    "            return f\"Error: {results[0]['error']}\\n{results[0].get('details', '')}\"\n",
    "\n",
    "        return '\\n\\n'.join(\n",
    "            f\"{item['title']}\\n\"\n",
    "            f\"Source:   {item['source']}\\n\"\n",
    "            f\"Date:     {item['published_at']}\\n\"\n",
    "            f\"URL:      {item['url']}\\n\"\n",
    "            f\"{item.get('description', 'No description')}\"\n",
    "            for item in results\n",
    "        )\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class RagTool(BaseTool):\n",
    "    name: str = 'rag_search'\n",
    "    description: str = 'RAG search with NewsAPI data. Returns most relevant chunks.'\n",
    "\n",
    "    class RagConfig:\n",
    "        embed_model = model\n",
    "        tokenizer = tokenizer\n",
    "        dims = 768\n",
    "        chunk_size = 256\n",
    "        chunk_overlap = 20\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "        collection_name = \"news_articles\"\n",
    "\n",
    "    parameters: dict = {\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'queries': {\n",
    "                'type': 'list',\n",
    "                'description': 'New generated queries for multi-rag.',\n",
    "            },\n",
    "        },\n",
    "        'required': ['queries'],\n",
    "    }\n",
    "    \n",
    "    def call(self, args):\n",
    "        docs = []\n",
    "        rag_results = []\n",
    "        data = args['page_text']\n",
    "        multi_query = args['queries']\n",
    "\n",
    "        content = data\n",
    "\n",
    "        for idx, item_content in enumerate(content['text']):\n",
    "            text = self._join_words(item_content)\n",
    "            chunks = self.RagConfig.text_splitter.create_documents(\n",
    "                [text],\n",
    "                metadatas=[content['json_data'][idx]]\n",
    "            )\n",
    "            docs.extend(chunks)\n",
    "\n",
    "        embeddings = self._embed_documents(docs)\n",
    "\n",
    "        self.RagConfig.qdrant_client.recreate_collection(\n",
    "            collection_name=self.RagConfig.collection_name,\n",
    "            vectors_config=VectorParams(\n",
    "                size=self.RagConfig.dims,\n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.RagConfig.qdrant_client.upsert(\n",
    "            collection_name=self.RagConfig.collection_name,\n",
    "            points=[\n",
    "                {\n",
    "                    \"id\": i,\n",
    "                    \"vector\": emb.tolist(),\n",
    "                    \"payload\": {\"text\": doc.page_content, **doc.metadata}\n",
    "                }\n",
    "                for i, (emb, doc) in enumerate(embeddings)\n",
    "            ]\n",
    "        )\n",
    "        for query in multi_query:\n",
    "            rag_results.append(self.search(query=query, top_k=4))\n",
    "            \n",
    "        return rag_results\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5):\n",
    "        \"\"\"Perform a vector similarity search for a given query string.\"\"\"\n",
    "        # 1. Эмбеддим запрос\n",
    "        inputs = self.RagConfig.tokenizer(\n",
    "            query,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            output = self.RagConfig.embed_model(**inputs)\n",
    "\n",
    "        if hasattr(output, \"pooler_output\") and output.pooler_output is not None:\n",
    "            query_vector = output.pooler_output.squeeze(0).cpu().numpy()\n",
    "        else:\n",
    "            query_vector = output.last_hidden_state.mean(dim=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "        # 2. Поиск через Qdrant\n",
    "        results = self.RagConfig.qdrant_client.search(\n",
    "            collection_name=self.RagConfig.collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=top_k,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        # 3. Возвращаем текст + метаданные\n",
    "        return [\n",
    "            {\n",
    "                \"score\": hit.score,\n",
    "                \"text\": hit.payload.get(\"text\"),\n",
    "                \"meta\": {k: v for k, v in hit.payload.items() if k != \"text\"}\n",
    "            }\n",
    "            for hit in results\n",
    "        ]\n",
    "\n",
    "\n",
    "    def _embed_documents(self, docs):\n",
    "        results = []\n",
    "        for doc in docs:\n",
    "            inputs = self.RagConfig.tokenizer(\n",
    "                doc.page_content,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                padding=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                output = self.RagConfig.embed_model(**inputs)\n",
    "\n",
    "            if hasattr(output, \"pooler_output\") and output.pooler_output is not None:\n",
    "                emb = output.pooler_output.squeeze(0)\n",
    "            else:\n",
    "                emb = output.last_hidden_state.mean(dim=1).squeeze(0)\n",
    "\n",
    "\n",
    "            results.append((emb.cpu().numpy(), doc))\n",
    "        return results\n",
    "\n",
    "    def _join_words(self, sequence):\n",
    "        if isinstance(sequence, list):\n",
    "            return \" \".join(sequence)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "379d6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = RagTool()\n",
    "ff = asd.call(test_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc27214b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f090b7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'title': 'Banning teens from social media will not fix everything. Here’s why', 'source': 'The-independent.com', 'url': 'https://www.the-independent.com/life-style/health-and-families/social-media-ban-teenager-addiction-b2752278.html', 'published_at': '2025-05-16T09:35:10Z', 'description': 'Jasleen Chhabra, Vita Pilkington and Zac Seidler on the complexities of a social media ban for teenagers', 'image_url': 'https://static.the-independent.com/2025/02/25/15/11/iStock-2180811155.jpeg?trim=0,180,0,180&width=1200&height=800&crop=1200:800'}, page_content='Platforms such as TikTok, Snapchat and Instagram are where young people connect with friends and online communities, explore and express their identities, seek information, and find support for mental health struggles.')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "39e124d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Optional, Dict\n",
    "\n",
    "def extract_tool_call(response: str) -> Optional[Dict[str, Any]]:\n",
    "    json_part = response.split('</think>')[-1].strip()\n",
    "\n",
    "    try:\n",
    "        data = json.loads(json_part.strip())\n",
    "        if isinstance(data, dict) and \"name\" in data and \"arguments\" in data:\n",
    "            return {\n",
    "                \"name\": data[\"name\"],\n",
    "                \"arguments\": data[\"arguments\"]\n",
    "            }\n",
    "    except json.JSONDecodeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f948d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterator, List, Optional, Tuple, Union, Literal\n",
    "def is_tool_schema(obj: dict) -> bool:\n",
    "    if not isinstance(obj, dict):\n",
    "        return False\n",
    "\n",
    "    if 'name' not in obj or not isinstance(obj['name'], str):\n",
    "        return False\n",
    "    if 'description' not in obj or not isinstance(obj['description'], str):\n",
    "        return False\n",
    "\n",
    "    params = obj.get('parameters')\n",
    "    if not isinstance(params, dict):\n",
    "        return False\n",
    "    if params.get('type') != 'object':\n",
    "        return False\n",
    "    if 'properties' not in params or not isinstance(params['properties'], dict):\n",
    "        return False\n",
    "    if 'required' not in params or not isinstance(params['required'], list):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "    \n",
    "gs = NewsAPITool()\n",
    "import json\n",
    "rt = RagTool()\n",
    "\n",
    "def execute_tools(tool_call) -> list:\n",
    "    results = {}\n",
    "    tool_name = tool_call[\"name\"]\n",
    "    args = tool_call[\"arguments\"]\n",
    "\n",
    "    if isinstance(args, str):\n",
    "        try:\n",
    "            args = json.loads(args)\n",
    "        except Exception as e:\n",
    "            results = {\n",
    "                \"tool\": tool_name,\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Failed to parse arguments: {e}\"\n",
    "            }\n",
    "    try:\n",
    "        if tool_name == \"news_api_search\":\n",
    "            json_data, page_text = gs.call(args)\n",
    "            results = {\n",
    "                \"tool\": tool_name,\n",
    "                \"status\": \"success\",\n",
    "                \"json_data\": json_data,\n",
    "                \"text\": page_text,\n",
    "            }\n",
    "        elif tool_name == \"rag_search\":\n",
    "            rag_results = rt.call(args)\n",
    "            results = {\n",
    "                \"tool\": tool_name,\n",
    "                \"status\": \"success\",\n",
    "                \"data\": rag_results,\n",
    "            }\n",
    "        else:\n",
    "            results = {\n",
    "                \"tool\": tool_name,\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Unknown tool: {tool_name}\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        results = {\n",
    "            \"tool\": tool_name,\n",
    "            \"status\": \"error\",\n",
    "            \"message\": str(e)\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "fc712b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Generate ONLY this JSON structure ONLY when query is related for news searches, otherwise dont use it:\n",
    "{\n",
    "  \"name\": \"news_api_search\",\n",
    "  \"arguments\": {\n",
    "    \"query\": \"optimized_search_query\",\n",
    "    \"language\": \"en\",\n",
    "    \"max_results\": 3\n",
    "  }\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947e40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "269f8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentMemory(BaseModel):\n",
    "    # Logicaly it seems better to store this class as an item in the session store.\n",
    "    # We can have different configs for different memorys.\n",
    "    memory_buffer: List[Message] = Field(default_factory=list)\n",
    "    max_memory_size: int = Field(default=10)\n",
    "\n",
    "class AgentSessionTracker(BaseModel):\n",
    "    session_id: str = Field(..., description=\"Unique session ID\")\n",
    "    user_id: str = Field(default_factory=str)\n",
    "    session_store: Dict[str, List] = Field(default_factory=dict)\n",
    "    \n",
    "    # Initializing the agent memory\n",
    "    agent_memory: AgentMemory = Field(default_factory=AgentMemory)\n",
    "    \n",
    "    @property\n",
    "    def memory_buffer(self) -> List[Message]:\n",
    "        if self.agent_memory.memory_buffer is None:\n",
    "            print(f\"[Session: {self.session_id}] Memory buffer was empty. Creating a new one.\")\n",
    "            self.agent_memory.memory_buffer = []\n",
    "        return self.agent_memory.memory_buffer\n",
    "\n",
    "    def _check_memory_size(self):\n",
    "        while len(self.memory_buffer) > self.agent_memory.max_memory_size:\n",
    "            self.memory_buffer.pop(0)\n",
    "            raise Warning(\n",
    "                f\"[Session: {self.session_id}] Memory buffer size exceeded. \"\n",
    "                f\"Removing the oldest message to maintain the limit of {self.agent_memory.max_memory_size}.\"\n",
    "            )\n",
    "\n",
    "    def add_message(self, messages: Union[Message, List[Message]]):\n",
    "        if isinstance(messages, list):\n",
    "            if not all(isinstance(m, Message) for m in messages):\n",
    "                raise ValueError(\"All items in the list must be of type Message.\")\n",
    "        elif isinstance(messages, Message):\n",
    "            messages = [messages]\n",
    "        else:\n",
    "            raise ValueError(\"'messages' must be of type Message or List[Message].\")\n",
    "        self.memory_buffer.extend(messages)\n",
    "        self._check_memory_size()\n",
    "    \n",
    "    def clean_memory(self):\n",
    "        self.agent_memory.memory_buffer = []\n",
    "    \n",
    "    def get_by_session_id(self, session_id: str) -> Optional[AgentMemory]:\n",
    "        if session_id not in self.session_store:\n",
    "            self.session_store[session_id] = self.memory_buffer\n",
    "        return self.session_store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3f5e515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GGML_VERBOSE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f1155e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import copy\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, llm, max_steps=3):\n",
    "        self.llm = llm\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def _build_context(self, memory_buff: List[Message]):\n",
    "        return [msg.model_dump() for msg in memory_buff]\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        session_id = str(uuid.uuid4())\n",
    "        memory_tracker = AgentSessionTracker(session_id=session_id)\n",
    "        SYSTEM_PROMPT_MULTI_QUERY = None\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=SYSTEM_PROMPT, role=\"system\"),\n",
    "            UserMessage(content=query, role=\"user\"),\n",
    "        ]\n",
    "\n",
    "        for step in range(self.max_steps):\n",
    "            sys_message = []\n",
    "            memory_tracker.add_message(messages)\n",
    "            context = self._build_context(memory_tracker.memory_buffer)\n",
    "            \n",
    "            response = self.llm.create_chat_completion(\n",
    "                messages=context,\n",
    "                temperature=0,\n",
    "                max_tokens=1000,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "            )\n",
    "            content = response['choices'][0]['message']['content']\n",
    "            print(f\"\\nStep {step} response:\\n{content}\")\n",
    "            \n",
    "            if tool_call := extract_tool_call(content):\n",
    "                print(f\"🛠️ Executing tool: {tool_call['name']} with args: {tool_call['arguments']}\")\n",
    "\n",
    "                \n",
    "                \n",
    "                formatted_results = []\n",
    "                if tool_call[\"name\"] == \"news_api_search\":\n",
    "                    tool_results = execute_tools(tool_call)\n",
    "\n",
    "                    if tool_results[\"status\"] == \"success\":\n",
    "                        formatted = f\"Article content:\\n{tool_results['json_data']}\"\n",
    "                        formatted_results.append(formatted)\n",
    "\n",
    "                        # Подготовка промпта для rag_search\n",
    "                        MULTI_QUERY_CALL_PROMPT = \"\"\"\n",
    "                    You are an AI language model assistant. Your task is to generate five different versions of the given user query.\n",
    "\n",
    "                    You must respond with ONLY the following JSON format — do not add explanations, markdown, or natural language:\n",
    "\n",
    "                    {{\n",
    "                    \"name\": \"rag_search\",\n",
    "                    \"arguments\": {{\n",
    "                        \"queries\": [\n",
    "                        \"reformulated_query_1\",\n",
    "                        \"reformulated_query_2\",\n",
    "                        \"reformulated_query_3\",\n",
    "                        \"reformulated_query_4\",\n",
    "                        \"reformulated_query_5\"\n",
    "                        ]\n",
    "                    }}\n",
    "                    }}\n",
    "\n",
    "                    Original query: {query}\n",
    "                    \"\"\".strip()\n",
    "\n",
    "\n",
    "                        messages = [\n",
    "                            LlmMessage(content=content, role=\"assistant\", tool_calls=[tool_results]),\n",
    "                            UserMessage(content=\"Tool execution results:\\n\" + \"\\n\\n\".join(formatted_results), role=\"user\"),\n",
    "                            SystemMessage(content=MULTI_QUERY_CALL_PROMPT.format(query=query), role=\"system\")\n",
    "                        ]\n",
    "                        \n",
    "                if tool_call[\"name\"] == \"rag_search\":\n",
    "                    rag_call = copy.deepcopy(tool_call)\n",
    "                    rag_call['arguments']['page_text'] = tool_results\n",
    "                    tool_results = execute_tools(rag_call)\n",
    "                    if tool_results[\"status\"] == \"success\":\n",
    "                        formatted_results.append(tool_results['data'])\n",
    "                \n",
    "            else:\n",
    "                return tool_results\n",
    "\n",
    "        return tool_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "090ef2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ags = Agent(llm=llm, max_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b2d29ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"give me the latest insights from crypto's and stock's world\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f72716",
   "metadata": {},
   "source": [
    "### Tool analyzer \n",
    "Add TOOL analyzer, TOOL tracker\n",
    "we need to track tool calling for the sake of logging\n",
    "When model need to anylyze answers from buffer, it needs to detect tool easyly just by key 'Role' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "796c9f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 85 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     857.19 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    5095.82 ms /    37 runs   (  137.72 ms per token,     7.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    5988.82 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0 response:\n",
      "{\"name\": \"news_api_search\", \"arguments\": {\"query\": \"cryptocurrency and stock market latest insights\", \"language\": \"en\", \"max_results\": 3}}\n",
      "\n",
      "🛠️ Executing tool: news_api_search with args: {'query': 'cryptocurrency and stock market latest insights', 'language': 'en', 'max_results': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 122 prefix-match hit, remaining 690 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     857.19 ms\n",
      "llama_perf_context_print: prompt eval time =    1843.17 ms /   690 tokens (    2.67 ms per token,   374.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7818.14 ms /    60 runs   (  130.30 ms per token,     7.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   11032.06 ms /   750 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1 response:\n",
      "{\"name\": \"rag_search\", \"arguments\": {\"queries\": [\"latest cryptocurrency and stock market insights\", \"recent updates on crypto and stock trends\", \"current developments in crypto and stock markets\", \"newest news about cryptocurrencies and stocks\", \"up-to-date analysis of crypto and stock world\"]}}\n",
      "\n",
      "🛠️ Executing tool: rag_search with args: {'queries': ['latest cryptocurrency and stock market insights', 'recent updates on crypto and stock trends', 'current developments in crypto and stock markets', 'newest news about cryptocurrencies and stocks', 'up-to-date analysis of crypto and stock world']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14178/605128883.py:223: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  self.RagConfig.qdrant_client.recreate_collection(\n",
      "/tmp/ipykernel_14178/605128883.py:267: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = self.RagConfig.qdrant_client.search(\n"
     ]
    }
   ],
   "source": [
    "mbmb = ags.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "701f0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_message = LlmMessage(content=\"Hello\", role=\"assistant\", tool_calls=[mbmb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b713cc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.56748736,\n",
       "  'text': 'Google Finance supports real-time data for popular cryptocurrencies like Bitcoin, Ethereum, and Solana, along with major fiat currencies like the USD, EUR, and GBP.',\n",
       "  'meta': {'title': 'How Google Finance Can Help You Manage Your Investments in 2025',\n",
       "   'source': 'Typeforyou.org',\n",
       "   'url': 'https://typeforyou.org/google-finance/',\n",
       "   'published_at': '2025-05-11T16:00:08Z',\n",
       "   'description': 'In today’s dynamic financial landscape, staying on top of your investments is more important than ever. With markets constantly shifting and new asset classes like...\\nThe post How Google Finance Can Help You Manage Your Investments in 2025 first appeared on T…',\n",
       "   'image_url': 'https://typeforyou.org/wp-content/uploads/2025/05/Screenshot-2025-05-11-180531.png'}},\n",
       " {'score': 0.5485753,\n",
       "  'text': 'In today’s dynamic financial landscape, staying on top of your investments is more important than ever. With markets constantly shifting and new asset classes like cryptocurrency gaining traction, having the right tools can make all the difference. That’s',\n",
       "  'meta': {'title': 'How Google Finance Can Help You Manage Your Investments in 2025',\n",
       "   'source': 'Typeforyou.org',\n",
       "   'url': 'https://typeforyou.org/google-finance/',\n",
       "   'published_at': '2025-05-11T16:00:08Z',\n",
       "   'description': 'In today’s dynamic financial landscape, staying on top of your investments is more important than ever. With markets constantly shifting and new asset classes like...\\nThe post How Google Finance Can Help You Manage Your Investments in 2025 first appeared on T…',\n",
       "   'image_url': 'https://typeforyou.org/wp-content/uploads/2025/05/Screenshot-2025-05-11-180531.png'}},\n",
       " {'score': 0.5278723,\n",
       "  'text': 'What started as a simple stock tracking tool has grown into a robust, user-friendly platform packed with powerful features. In 2025, Google Finance helps everyone from beginners to seasoned investors monitor markets, manage portfolios, and make smarter',\n",
       "  'meta': {'title': 'How Google Finance Can Help You Manage Your Investments in 2025',\n",
       "   'source': 'Typeforyou.org',\n",
       "   'url': 'https://typeforyou.org/google-finance/',\n",
       "   'published_at': '2025-05-11T16:00:08Z',\n",
       "   'description': 'In today’s dynamic financial landscape, staying on top of your investments is more important than ever. With markets constantly shifting and new asset classes like...\\nThe post How Google Finance Can Help You Manage Your Investments in 2025 first appeared on T…',\n",
       "   'image_url': 'https://typeforyou.org/wp-content/uploads/2025/05/Screenshot-2025-05-11-180531.png'}},\n",
       " {'score': 0.5234861,\n",
       "  'text': 'Compare multiple stocks or indices to analyze relative performance and diversification.\\nNew AI-powered tools in 2025 show bullish or bearish sentiment based on news volume and social media signals.',\n",
       "  'meta': {'title': 'How Google Finance Can Help You Manage Your Investments in 2025',\n",
       "   'source': 'Typeforyou.org',\n",
       "   'url': 'https://typeforyou.org/google-finance/',\n",
       "   'published_at': '2025-05-11T16:00:08Z',\n",
       "   'description': 'In today’s dynamic financial landscape, staying on top of your investments is more important than ever. With markets constantly shifting and new asset classes like...\\nThe post How Google Finance Can Help You Manage Your Investments in 2025 first appeared on T…',\n",
       "   'image_url': 'https://typeforyou.org/wp-content/uploads/2025/05/Screenshot-2025-05-11-180531.png'}}]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbmb['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198d4880",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'json_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[232]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmbmb\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjson_data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'json_data'"
     ]
    }
   ],
   "source": [
    "mbmb['js']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9eba4b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={}, page_content='B'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='u'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='S'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='g'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='&'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='m'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content=';'),\n",
       "  Document(metadata={}, page_content='T'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='E'),\n",
       "  Document(metadata={}, page_content='S'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='B'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='u'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='L'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='O'),\n",
       "  Document(metadata={}, page_content='f'),\n",
       "  Document(metadata={}, page_content='H'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='A'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='D'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='m'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='T'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='u'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='g'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='w'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='C'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='-'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content='L'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='v'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='M'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content=','),\n",
       "  Document(metadata={}, page_content='E'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='g'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='.'),\n",
       "  Document(metadata={}, page_content='B'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='B'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='u'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='S'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='g'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='m'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='g'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='b'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='k'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='…'),\n",
       "  Document(metadata={}, page_content='['),\n",
       "  Document(metadata={}, page_content='+'),\n",
       "  Document(metadata={}, page_content='4'),\n",
       "  Document(metadata={}, page_content='6'),\n",
       "  Document(metadata={}, page_content='2'),\n",
       "  Document(metadata={}, page_content='3'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content=']')],\n",
       " [Document(metadata={}, page_content='A'),\n",
       "  Document(metadata={}, page_content='g'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='u'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='f'),\n",
       "  Document(metadata={}, page_content='D'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='m'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='v'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='N'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='w'),\n",
       "  Document(metadata={}, page_content='Y'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='k'),\n",
       "  Document(metadata={}, page_content='T'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='m'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='O'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='y'),\n",
       "  Document(metadata={}, page_content='w'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='y'),\n",
       "  Document(metadata={}, page_content=','),\n",
       "  Document(metadata={}, page_content='w'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='m'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='\"'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='\"'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='\"'),\n",
       "  Document(metadata={}, page_content='u'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='.'),\n",
       "  Document(metadata={}, page_content='\"'),\n",
       "  Document(metadata={}, page_content='\"'),\n",
       "  Document(metadata={}, page_content='Y'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='u'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='g'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='b'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='k'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='w'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='g'),\n",
       "  Document(metadata={}, page_content='b'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='…'),\n",
       "  Document(metadata={}, page_content='['),\n",
       "  Document(metadata={}, page_content='+'),\n",
       "  Document(metadata={}, page_content='3'),\n",
       "  Document(metadata={}, page_content='2'),\n",
       "  Document(metadata={}, page_content='7'),\n",
       "  Document(metadata={}, page_content='5'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content=']')],\n",
       " [Document(metadata={}, page_content='R'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='k'),\n",
       "  Document(metadata={}, page_content='m'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='g'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='B'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='u'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='S'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='g'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='m'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='f'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='m'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='k'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='b'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='u'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='U'),\n",
       "  Document(metadata={}, page_content='S'),\n",
       "  Document(metadata={}, page_content='P'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='D'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='T'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='u'),\n",
       "  Document(metadata={}, page_content='m'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='p'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='f'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='m'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content=','),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='g'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='A'),\n",
       "  Document(metadata={}, page_content='m'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='l'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='d'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='u'),\n",
       "  Document(metadata={}, page_content='n'),\n",
       "  Document(metadata={}, page_content='f'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='f'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='o'),\n",
       "  Document(metadata={}, page_content='f'),\n",
       "  Document(metadata={}, page_content='f'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='.'),\n",
       "  Document(metadata={}, page_content='T'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='e'),\n",
       "  Document(metadata={}, page_content='m'),\n",
       "  Document(metadata={}, page_content='i'),\n",
       "  Document(metadata={}, page_content='g'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='t'),\n",
       "  Document(metadata={}, page_content='y'),\n",
       "  Document(metadata={}, page_content='…'),\n",
       "  Document(metadata={}, page_content='['),\n",
       "  Document(metadata={}, page_content='+'),\n",
       "  Document(metadata={}, page_content='3'),\n",
       "  Document(metadata={}, page_content='4'),\n",
       "  Document(metadata={}, page_content='1'),\n",
       "  Document(metadata={}, page_content='5'),\n",
       "  Document(metadata={}, page_content='c'),\n",
       "  Document(metadata={}, page_content='h'),\n",
       "  Document(metadata={}, page_content='a'),\n",
       "  Document(metadata={}, page_content='r'),\n",
       "  Document(metadata={}, page_content='s'),\n",
       "  Document(metadata={}, page_content=']')]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radi_boga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf50ef",
   "metadata": {},
   "source": [
    "## TOOL IDEA\n",
    "code tool that will take only hot key moments from the news page, and then give to a user Connection beatween hot-takes and the actual page.\n",
    "\n",
    "\n",
    "### Check notes about storing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c18d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.create_chat_completion(\n",
    "    messages=messages,\n",
    "    temperature=0.1,\n",
    "    max_tokens=400,\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae19f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in response:\n",
    "    if 'choices' in chunk and len(chunk['choices']) > 0:\n",
    "        content = chunk['choices'][0]['delta'].get('content', '')\n",
    "        if content:\n",
    "            print(content, end='', flush=True)\n",
    "    else:\n",
    "        print(\"No valid response received.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f184ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae1dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = LlmMessage(\n",
    "    role=\"assistant\",\n",
    "    content=\"I am an AI assistant. How can I help you?\",\n",
    ")\n",
    "sss2 = UserMessage(\n",
    "    role=\"user\",\n",
    "    content=\"What is the latest update on stock market trends?\",\n",
    ")\n",
    "sss3 = SystemMessage(\n",
    "    role=\"system\",\n",
    "    content=\"Generate ONLY this JSON structure ONLY when query is related for news searches, otherwise dont use it: {\\\"name\\\": \\\"news_api_search\\\", \\\"arguments\\\": {\\\"query\\\": \\\"optimized_search_query\\\", \\\"language\\\": \\\"en\\\", \\\"max_results\\\": 3}}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca2348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'system',\n",
       " 'content': 'Generate ONLY this JSON structure ONLY when query is related for news searches, otherwise dont use it:\\n{\\n  \"name\": \"news_api_search\",\\n  \"arguments\": {\\n    \"query\": \"optimized_search_query\",\\n    \"language\": \"en\",\\n    \"max_results\": 3\\n  }\\n}'}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SystemMessage(content=SYSTEM_PROMPT, role=\"system\").model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_1 = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n",
    "message_2 = {\"role\": \"user\", \"content\": \"What is the latest update on stock market trends?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aaf688",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.create_chat_completion(\n",
    "    messages=[sss3.model_dump(), sss.model_dump(), sss2.model_dump()],\n",
    "    temperature=0.1,\n",
    "    max_tokens=400,\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09946935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Generate ONLY this JSON structure ONLY when query is related for news searches, otherwise dont use it: {\"name\": \"news_api_search\", \"arguments\": {\"query\": \"optimized_search_query\", \"language\": \"en\", \"max_results\": 3}}<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I am an AI assistant. How can I help you?<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the latest update on stock market trends?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template([sss3.model_dump(), sss.model_dump(), sss2.model_dump()], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227099b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 21 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking about the latest update on stock market trends. I need to figure out how to respond. Let me check the instructions again. The user mentioned generating a JSON structure only when the query is related to news searches. Otherwise, don't use it.\n",
      "\n",
      "So, the query here is about stock market trends. That's a news-related topic. Therefore, I should generate the JSON structure. The JSON should have the name \"news_api_search\" with arguments including the query, language set to English, and max_results as 3. \n",
      "\n",
      "Wait, the user's query is \"latest update on stock market trends.\" I need to make sure the optimized_search_query is correctly formatted. Maybe use keywords like \"stock market trends latest\" to get the most relevant results. Also, check that the language is \"en\" and max_results is 3. \n",
      "\n",
      "I should structure the JSON exactly as specified. Let me double-check the syntax to avoid any errors. The keys should be in quotes, and the structure should be correct. Alright, that should do it.\n",
      "</think>\n",
      "\n",
      "{\"name\": \"news_api_search\", \"arguments\": {\"query\": \"stock market trends latest\", \"language\": \"en\", \"max_results\": 3}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     859.90 ms\n",
      "llama_perf_context_print: prompt eval time =     734.98 ms /    70 tokens (   10.50 ms per token,    95.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38535.33 ms /   253 runs   (  152.31 ms per token,     6.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   39683.94 ms /   323 tokens\n"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    if 'choices' in chunk and len(chunk['choices']) > 0:\n",
    "        content = chunk['choices'][0]['delta'].get('content', '')\n",
    "        if content:\n",
    "            print(content, end='', flush=True)\n",
    "    else:\n",
    "        print(\"No valid response received.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221cf59c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
